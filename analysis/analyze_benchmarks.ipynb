#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEV‑SNP Benchmarks – Analysis Pipeline
-------------------------------------

Compares benchmark results with AMD SEV‑SNP memory encryption enabled (`sev_on`)
versus disabled (`sev_off`).

Inputs
------
benchmark/
    sev_on/cpu/*.log
    sev_on/ram/*.log
    sev_off/cpu/*.log
    sev_off/ram/*.log

Outputs (written to analysis_output/)
-------------------------------------
raw_results.csv
descriptive_stats.csv
significance_tests.csv
violin_swarm.png
hist_cpu_perf.png
hist_ram_bw_mib_s.png
ridge.png               (optional, requires joypy)
qq_cpu_perf.png
qq_ram_bw_mib_s.png
"""

import re
import pathlib
import warnings
from typing import List, Dict

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as st
import statsmodels.api as sm
import pingouin as pg

# Optional ridge‑plot
try:
    import joypy

    JOYPY_OK = True
except ImportError:
    JOYPY_OK = False
    print("⚠ joypy not installed – ridge plot will be skipped")

# --------------------------------------------------------------------------- #
# Configuration
# --------------------------------------------------------------------------- #
BASE_DIR: pathlib.Path = pathlib.Path.cwd() / "benchmark"      # root folder for logs
OUT_DIR: pathlib.Path = pathlib.Path("analysis_output")
OUT_DIR.mkdir(exist_ok=True)

warnings.filterwarnings("ignore")
sns.set_theme(style="whitegrid")


# --------------------------------------------------------------------------- #
# Helper functions for parsing sysbench stdout
# --------------------------------------------------------------------------- #

def parse_sysbench_cpu(text: str) -> float:
    """
    Extract 'events per second' from `sysbench cpu` output.

    If that line is missing, fall back to the inverse of the total runtime,
    so that 'higher is better' holds.

    Returns
    -------
    float
        events per second, or np.nan if nothing could be extracted
    """
    match = re.search(r"events per second:\s*([\d.]+)", text)
    if match:
        return float(match.group(1))

    match = re.search(r"total time:\s*([\d.]+)s", text)
    if match:
        return 1.0 / float(match.group(1))

    return np.nan


def parse_sysbench_mem(text: str) -> float:
    """
    Extract memory bandwidth (MiB/sec) from `sysbench memory` output.

    Returns np.nan when no pattern matches.
    """
    patterns = [
        r"MiB transferred \(([\d.]+) MiB/sec\)",
        r"MiB/sec\)\s*=\s*([\d.]+)",
        r"MiB\/sec\):\s*([\d.]+)",
    ]
    for pat in patterns:
        match = re.search(pat, text)
        if match:
            return float(match.group(1))
    return np.nan


# --------------------------------------------------------------------------- #
# Main analysis workflow
# --------------------------------------------------------------------------- #

def load_results() -> pd.DataFrame:
    """Walk the BASE_DIR and parse all .log files."""
    rows: List[Dict] = []
    for mode in ("sev_on", "sev_off"):
        for domain in ("cpu", "ram"):
            for log_file in (BASE_DIR / mode / domain).rglob("*.log"):
                text = log_file.read_text(errors="ignore")
                if domain == "cpu":
                    value = parse_sysbench_cpu(text)
                    metric = "cpu_perf"
                else:
                    value = parse_sysbench_mem(text)
                    metric = "ram_bw_mib_s"

                if np.isnan(value):
                    continue

                rows.append(
                    {
                        "scenario": mode,
                        "domain": domain,
                        "metric": metric,
                        "value": value,
                        "file": str(log_file),
                    }
                )

    df = pd.DataFrame(rows)
    print(f"✔ Loaded {len(df)} data points")
    return df


def descriptive_stats(df: pd.DataFrame) -> pd.DataFrame:
    """Compute count, mean, median, std, IQR, SEM and 95 % CI per scenario/metric."""
    if df.empty:
        raise ValueError("No measurements loaded – check BASE_DIR and parsing patterns.")

    desc = (
        df.groupby(["scenario", "metric"])
        .value.agg(
            [
                "count",
                "mean",
                "median",
                "std",
                "min",
                "max",
                ("iqr", lambda x: st.iqr(x)),
                ("sem", st.sem),
            ]
        )
        .assign(ci95=lambda d: 1.96 * d["sem"])
        .round(3)
        .reset_index()
    )
    return desc


def violin_swarm(df: pd.DataFrame) -> None:
    """Combined violin + swarm plot."""
    plt.figure(figsize=(7, 5))
    sns.violinplot(data=df, x="scenario", y="value", hue="metric", inner=None, split=True)
    sns.swarmplot(data=df, x="scenario", y="value", hue="metric", dodge=True, size=3, palette="dark")
    plt.title("Distribution of CPU & RAM performance – SEV off vs. on")
    plt.tight_layout()
    plt.savefig(OUT_DIR / "violin_swarm.png")
    plt.show()


def histograms(df: pd.DataFrame) -> None:
    """Histogram + KDE per metric."""
    mapping = [("cpu_perf", "CPU events/s"), ("ram_bw_mib_s", "RAM bandwidth MiB/s")]
    for metric, label in mapping:
        plt.figure(figsize=(6, 4))
        sns.histplot(
            data=df[df.metric == metric],
            x="value",
            hue="scenario",
            kde=True,
            stat="density",
            common_norm=False,
            alpha=0.4,
        )
        plt.title(f"Histogram & KDE – {label}")
        plt.tight_layout()
        plt.savefig(OUT_DIR / f"hist_{metric}.png")
        plt.show()


def ridge_plot(df: pd.DataFrame) -> None:
    """Optional ridge plot if joypy is installed."""
    if not JOYPY_OK:
        return
    order = ["sev_off", "sev_on"]
    joypy.joyplot(df.set_index("scenario").loc[order].reset_index(), by="scenario", column="value", overlap=0.4, figsize=(6, 4))
    plt.title("Ridge plot of all measurements")
    plt.tight_layout()
    plt.savefig(OUT_DIR / "ridge.png")
    plt.show()


def qq_plots(df: pd.DataFrame) -> None:
    """QQ-plots per metric."""
    for metric in df.metric.unique():
        plt.figure(figsize=(4, 4))
        sm.qqplot(df[df.metric == metric].value, line="s")
        plt.title(f"QQ‑plot {metric}")
        plt.tight_layout()
        plt.savefig(OUT_DIR / f"qq_{metric}.png")
        plt.show()


def significance_tests(df: pd.DataFrame) -> pd.DataFrame:
    """Shapiro, Levene, t‑test / Mann‑Whitney + effect sizes."""
    rows = []
    for metric in df.metric.unique():
        a = df.query("scenario == 'sev_on' and metric == @metric").value
        b = df.query("scenario == 'sev_off' and metric == @metric").value

        norm_a, norm_b = st.shapiro(a).pvalue, st.shapiro(b).pvalue
        var_eq = st.levene(a, b).pvalue > 0.05

        if norm_a > 0.05 and norm_b > 0.05 and var_eq:
            test_name = "Welch t‑test"
            stat, pval = st.ttest_ind(a, b, equal_var=False)
            effect = pg.compute_effsize(a, b, eftype="cohen")
        else:
            test_name = "Mann‑Whitney U"
            stat, pval = st.mannwhitneyu(a, b, alternative="two-sided")
            effect = pg.compute_effsize(a, b, eftype="r")

        rows.append([metric, test_name, stat, pval, effect])

    stats = pd.DataFrame(rows, columns=["metric", "test", "statistic", "p_value", "effect_size"])
    return stats


def textual_interpretation(desc: pd.DataFrame, stats: pd.DataFrame) -> None:
    """Human‑readable summary printed to stdout."""
    print("\n----------------  Interpretation  ----------------")
    for metric in ["cpu_perf", "ram_bw_mib_s"]:
        mean_off = desc.query("scenario == 'sev_off' and metric == @metric")["mean"].iat[0]
        mean_on = desc.query("scenario == 'sev_on' and metric == @metric")["mean"].iat[0]
        diff = mean_off - mean_on
        pct = diff / mean_off * 100

        significant = "significant" if stats.loc[stats.metric == metric, "p_value"].iat[0] < 0.05 else "not significant"
        label = "CPU performance" if metric == "cpu_perf" else "RAM bandwidth"
        slower = "slower" if metric == "cpu_perf" else "lower"

        effect = stats.loc[stats.metric == metric, "effect_size"].iat[0]

        print(
            f"{label}: SEV‑ON is {diff:.2f} ({pct:.2f}\u202f%) {slower} than SEV‑OFF "
            f"→ Difference {significant}, effect size {effect:.2f}."
        )

    print(f"\n✓ All figures & tables saved to {OUT_DIR.resolve()}")


def main() -> None:
    df = load_results()
    df.to_csv(OUT_DIR / "raw_results.csv", index=False)

    desc = descriptive_stats(df)
    desc.to_csv(OUT_DIR / "descriptive_stats.csv", index=False)

    violin_swarm(df)
    histograms(df)
    ridge_plot(df)
    qq_plots(df)

    stats = significance_tests(df)
    stats.to_csv(OUT_DIR / "significance_tests.csv", index=False)

    textual_interpretation(desc, stats)


if __name__ == "__main__":
    main()
